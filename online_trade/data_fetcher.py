"""
åœ¨çº¿äº¤æ˜“æ•°æ®æ‹‰å–å™¨
æ¯å°æ—¶å®šæ—¶æ‹‰å–æœ€æ–°çš„Kçº¿æ•°æ®
"""

import os
import sys
import pandas as pd
import requests
import time
from datetime import datetime, timedelta
import pytz
import schedule
from enum import Enum

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from utils.log_utils import print_log
from online_trade.config_loader import get_config
from online_trade.log_manager import get_log_manager

class DataType(Enum):
    SPOT = "spot"

# æ—¶åŒºè®¾ç½®
CHINA_TZ = pytz.timezone('Asia/Shanghai')

# APIé…ç½®
BINANCE_API_URL = "https://api.binance.com"
REQUEST_TIMEOUT = 30

# è®¾ç½®pandasé€‰é¡¹
pd.set_option('display.float_format', '{:.8f}'.format)

class OnlineDataFetcher:
    """åœ¨çº¿æ•°æ®æ‹‰å–å™¨"""
    
    def __init__(self, data_dir="../online_data", lookback_days=21):
        """
        åˆå§‹åŒ–æ•°æ®æ‹‰å–å™¨
        
        å‚æ•°:
        - data_dir: æ•°æ®å­˜å‚¨ç›®å½•
        - lookback_days: å›çœ‹å¤©æ•°ï¼Œç”¨äºåˆå§‹åŒ–å’Œè¡¥å……æ•°æ®
        """
        self.data_dir = data_dir
        self.lookback_days = lookback_days
        self.spot_data_dir = os.path.join(data_dir, "spot_klines")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.spot_data_dir, exist_ok=True)
        
        # åˆå§‹åŒ–æ•°æ®æ‹‰å–ä¸“ç”¨æ—¥å¿—ç®¡ç†å™¨
        from online_trade.log_manager import LogManager
        self.logger = LogManager(base_dir="dat_log", enable_console=True)
        self.logger.log_system_start("OnlineDataFetcher", {
            "data_dir": self.data_dir,
            "lookback_days": self.lookback_days
        })
        
        print(f"ğŸ“‚ æ•°æ®æ‹‰å–å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   æ•°æ®ç›®å½•: {self.data_dir}")
        print(f"   ç°è´§æ•°æ®ç›®å½•: {self.spot_data_dir}")
    
    def format_timestamp(self, date_time):
        """æ ¼å¼åŒ–æ—¶é—´æˆ³"""
        return date_time.strftime('%Y-%m-%d_%H_%M')
    
    def parse_time_str(self, time_str):
        """è§£ææ—¶é—´å­—ç¬¦ä¸²"""
        return datetime.strptime(time_str, '%Y-%m-%d_%H_%M').replace(tzinfo=CHINA_TZ)
    
    def get_klines_from_api(self, symbol, interval="15m", start_time=None, end_time=None, limit=5000):
        """
        ä»å¸å®‰APIè·å–Kçº¿æ•°æ®
        """
        url = f"{BINANCE_API_URL}/api/v3/klines"
        
        params = {
            'symbol': symbol,
            'interval': interval,
            'limit': limit
        }
        
        if start_time:
            if isinstance(start_time, datetime):
                start_time = int(start_time.timestamp() * 1000)
            params['startTime'] = start_time
            
        if end_time:
            if isinstance(end_time, datetime):
                end_time = int(end_time.timestamp() * 1000)
            params['endTime'] = end_time
        
        try:
            # è®°å½•APIè°ƒç”¨
            self.logger.log_api_call("OnlineDataFetcher", f"get_klines/{symbol}", params, success=True)
            
            response = requests.get(url, params=params, timeout=REQUEST_TIMEOUT)
            response.raise_for_status()
            
            data = response.json()
            if not data:
                self.logger.warning(f"APIè¿”å›ç©ºæ•°æ®: {symbol}", "OnlineDataFetcher")
                return None
                
            # è½¬æ¢ä¸ºDataFrame
            df = pd.DataFrame(data, columns=[
                'openTime', 'open', 'high', 'low', 'close', 'volumn',
                'closeTime', 'quote_volumn', 'count', 'taker_buy_volumn',
                'taker_buy_quote_volumn', 'ignore'
            ])
            
            # æ•°æ®ç±»å‹è½¬æ¢
            df['openTime'] = pd.to_datetime(df['openTime'], unit='ms')
            for col in ['open', 'high', 'low', 'close', 'volumn', 'quote_volumn']:
                df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # è®°å½•æ•°æ®æ‹‰å–æˆåŠŸ
            self.logger.log_data_fetch("OnlineDataFetcher", symbol, interval, len(df), success=True)
            
            return df
            
        except Exception as e:
            error_msg = str(e)
            print(f"âŒ è·å– {symbol} Kçº¿æ•°æ®å¤±è´¥: {error_msg}")
            self.logger.log_api_call("OnlineDataFetcher", f"get_klines/{symbol}", params, success=False, error_msg=error_msg)
            self.logger.log_data_fetch("OnlineDataFetcher", symbol, interval, 0, success=False, error_msg=error_msg)
            return None
    
    def load_existing_data(self, symbol):
        """åŠ è½½ç°æœ‰çš„æ•°æ®æ–‡ä»¶"""
        file_path = os.path.join(self.spot_data_dir, f"{symbol}_15m.csv")
        
        if os.path.exists(file_path):
            try:
                df = pd.read_csv(file_path)
                df['openTime'] = pd.to_datetime(df['openTime'])
                return df
            except Exception as e:
                print(f"âš ï¸  åŠ è½½ {symbol} ç°æœ‰æ•°æ®å¤±è´¥: {str(e)}")
                return None
        
        return None
    
    def save_data(self, symbol, df):
        """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
        if df is None or len(df) == 0:
            return False
            
        file_path = os.path.join(self.spot_data_dir, f"{symbol}_15m.csv")
        
        try:
            # æŒ‰æ—¶é—´æ’åºå¹¶å»é‡
            df = df.sort_values('openTime').drop_duplicates(subset=['openTime'], keep='last')
            df.to_csv(file_path, index=False)
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜ {symbol} æ•°æ®å¤±è´¥: {str(e)}")
            return False
    
    def update_symbol_data(self, symbol):
        """æ›´æ–°å•ä¸ªå¸ç§çš„æ•°æ®"""
        print(f"ğŸ”„ æ›´æ–° {symbol} æ•°æ®...")
        
        # åŠ è½½ç°æœ‰æ•°æ®
        existing_df = self.load_existing_data(symbol)
        
        if existing_df is not None and len(existing_df) > 0:
            # ä»æœ€åä¸€æ¡æ•°æ®çš„æ—¶é—´å¼€å§‹æ‹‰å–
            last_time = existing_df['openTime'].max()
            start_time = last_time + timedelta(minutes=15)  # é¿å…é‡å¤
        else:
            # æ²¡æœ‰ç°æœ‰æ•°æ®ï¼Œæ‹‰å–æœ€è¿‘30å¤©çš„æ•°æ®
            start_time = datetime.now(CHINA_TZ) - timedelta(days=self.lookback_days)
        
        # æ‹‰å–æœ€æ–°æ•°æ®
        new_df = self.get_klines_from_api(symbol, "15m", start_time=start_time)
        
        if new_df is None or len(new_df) == 0:
            print(f"âš ï¸  {symbol} æ²¡æœ‰æ–°æ•°æ®")
            return False
        
        # åˆå¹¶æ•°æ®
        if existing_df is not None and len(existing_df) > 0:
            combined_df = pd.concat([existing_df, new_df], ignore_index=True)
        else:
            combined_df = new_df
        
        # ä¿å­˜æ•°æ®
        success = self.save_data(symbol, combined_df)
        if success:
            print(f"âœ… {symbol} æ•°æ®æ›´æ–°æˆåŠŸï¼Œå…± {len(combined_df)} æ¡è®°å½•")
        
        return success
    
    def load_symbol_list(self):
        """åŠ è½½å¸ç§åˆ—è¡¨"""
        symbol_file = os.path.join(self.data_dir, "exchange_binance_market.txt")
        
        try:
            with open(symbol_file, 'r', encoding='utf-8') as f:
                symbols = [line.strip() for line in f.readlines() if line.strip()]
            print(f"ğŸ“‹ åŠ è½½äº† {len(symbols)} ä¸ªå¸ç§")
            return symbols
        except Exception as e:
            print(f"âŒ åŠ è½½å¸ç§åˆ—è¡¨å¤±è´¥: {str(e)}")
            return []
    
    def fetch_all_data(self):
        """æ‹‰å–æ‰€æœ‰å¸ç§çš„æ•°æ®"""
        print("ğŸš€ å¼€å§‹æ‹‰å–æ‰€æœ‰å¸ç§æ•°æ®...")
        
        symbols = self.load_symbol_list()
        if not symbols:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°å¸ç§åˆ—è¡¨")
            return
        
        success_count = 0
        failed_symbols = []
        
        for i, symbol in enumerate(symbols):
            try:
                success = self.update_symbol_data(symbol)
                if success:
                    success_count += 1
                else:
                    failed_symbols.append(symbol)
                
                # è¿›åº¦æ˜¾ç¤º
                if (i + 1) % 10 == 0:
                    print(f"ğŸ“Š è¿›åº¦: {i + 1}/{len(symbols)} ({(i + 1)/len(symbols)*100:.1f}%)")
                
                # APIé™åˆ¶ï¼Œé€‚å½“å»¶è¿Ÿ
                time.sleep(0.1)
                
            except Exception as e:
                print(f"âŒ å¤„ç† {symbol} æ—¶å‡ºé”™: {str(e)}")
                failed_symbols.append(symbol)
        
        print("\n" + "="*60)
        print("ğŸ‰ æ•°æ®æ‹‰å–å®Œæˆ!")
        print("="*60)
        print(f"ğŸ“Š ç»Ÿè®¡ç»“æœ:")
        print(f"   âœ… æˆåŠŸ: {success_count} ä¸ªå¸ç§")
        print(f"   âŒ å¤±è´¥: {len(failed_symbols)} ä¸ªå¸ç§")
        
        if failed_symbols:
            print(f"âŒ å¤±è´¥å¸ç§: {', '.join(failed_symbols[:10])}")
            if len(failed_symbols) > 10:
                print(f"   ... è¿˜æœ‰ {len(failed_symbols) - 10} ä¸ª")
    
    def start_scheduler(self):
        """å¯åŠ¨å®šæ—¶ä»»åŠ¡"""
        self.logger.info("å¯åŠ¨æ•°æ®æ‹‰å–å®šæ—¶ä»»åŠ¡", "OnlineDataFetcher")
        print("â° å¯åŠ¨æ•°æ®æ‹‰å–å®šæ—¶ä»»åŠ¡...")
        print("ğŸ“… è°ƒåº¦è§„åˆ™: æ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡")
        
        # ç«‹å³æ‰§è¡Œä¸€æ¬¡
        print("ğŸ”¥ ç«‹å³æ‰§è¡Œä¸€æ¬¡æ•°æ®æ‹‰å–...")
        self.logger.info("ç«‹å³æ‰§è¡Œæ•°æ®æ‹‰å–", "OnlineDataFetcher")
        self.fetch_all_data()
        
        # è®¾ç½®å®šæ—¶ä»»åŠ¡ï¼šæ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡
        schedule.every().hour.do(self.fetch_all_data)
        
        print("âœ… å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨ï¼Œç­‰å¾…æ‰§è¡Œ...")
        self.logger.info("å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨", "OnlineDataFetcher")
        
        while True:
            try:
                schedule.run_pending()
                time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            except KeyboardInterrupt:
                self.logger.info("æ•°æ®æ‹‰å–å™¨åœæ­¢ï¼šç”¨æˆ·ä¸­æ–­", "OnlineDataFetcher")
                print("\nâ¹ï¸  æ•°æ®æ‹‰å–å™¨å·²åœæ­¢")
                break
            except Exception as e:
                self.logger.error(f"å®šæ—¶ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {str(e)}", "OnlineDataFetcher", exc_info=True)
                print(f"âŒ å®šæ—¶ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {str(e)}")
                time.sleep(60)  # ç­‰å¾…ä¸€åˆ†é’Ÿåç»§ç»­


def run_data_fetcher():
    """è¿è¡Œæ•°æ®æ‹‰å–å™¨"""
    fetcher = OnlineDataFetcher()
    fetcher.fetch_all_data()


def start_scheduler():
    """å¯åŠ¨å®šæ—¶ä»»åŠ¡"""
    print("â° å¯åŠ¨æ•°æ®æ‹‰å–å®šæ—¶ä»»åŠ¡...")
    print("ğŸ“… è°ƒåº¦è§„åˆ™: æ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡")
    
    # ç«‹å³æ‰§è¡Œä¸€æ¬¡
    print("ğŸ”¥ ç«‹å³æ‰§è¡Œä¸€æ¬¡æ•°æ®æ‹‰å–...")
    run_data_fetcher()
    
    # è®¾ç½®å®šæ—¶ä»»åŠ¡ï¼šæ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡
    schedule.every().hour.do(run_data_fetcher)
    
    print("âœ… å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨ï¼Œç­‰å¾…æ‰§è¡Œ...")
    
    while True:
        schedule.run_pending()
        time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='åœ¨çº¿æ•°æ®æ‹‰å–å™¨')
    parser.add_argument('--mode', choices=['once', 'schedule'], 
                        default='once', help='è¿è¡Œæ¨¡å¼ï¼šonce=å•æ¬¡æ‰§è¡Œï¼Œschedule=å®šæ—¶æ‰§è¡Œ')
    
    args = parser.parse_args()
    
    if args.mode == 'schedule':
        start_scheduler()
    else:
        run_data_fetcher()
